import os
from pathlib import Path
import re
import time
from dotenv import load_dotenv
from openai import OpenAI
from pymilvus import connections, Collection, utility
import tiktoken
import numpy as np

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv('OPEN_API_KEY')

# Kh·ªüi t·∫°o OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

# Milvus connection parameters
MILVUS_HOST = 'localhost'
MILVUS_PORT = '19530'

def parse_website_crawl_file(file_path):
    """Parse file crawl website v√† tr√≠ch xu·∫•t th√¥ng tin t·ª´ng trang."""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # T√°ch c√°c trang b·∫±ng d·∫•u ph√¢n c√°ch
    pages = content.split('=' * 80)
    
    parsed_pages = []
    
    for page in pages[1:]:  # B·ªè qua ph·∫ßn header ƒë·∫ßu ti√™n
        if not page.strip():
            continue
            
        lines = page.strip().split('\n')
        if len(lines) < 3:
            continue
            
        # Parse th√¥ng tin trang
        page_info = {}
        
        # D√≤ng ƒë·∫ßu ti√™n ch·ª©a [TRANG X/Y] T√äN_TRANG
        title_line = lines[0].strip()
        title_match = re.match(r'\[TRANG (\d+)/(\d+)\] (.+)', title_line)
        if title_match:
            page_info['page_number'] = int(title_match.group(1))
            page_info['total_pages'] = int(title_match.group(2))
            page_info['page_title'] = title_match.group(3)
        
        # T√¨m URL
        url_line = None
        length_line = None
        content_start = 3  # M·∫∑c ƒë·ªãnh n·ªôi dung b·∫Øt ƒë·∫ßu t·ª´ d√≤ng th·ª© 4
        
        for i, line in enumerate(lines[1:], 1):
            if line.startswith('URL:'):
                url_line = line
                page_info['url'] = line.replace('URL:', '').strip()
            elif line.startswith('ƒê·ªô d√†i:'):
                length_line = line
                page_info['length'] = line.replace('ƒê·ªô d√†i:', '').strip()
            elif line.startswith('--'):
                content_start = i + 1
                break
        
        # L·∫•y n·ªôi dung trang
        if content_start < len(lines):
            page_content = '\n'.join(lines[content_start:]).strip()
            page_info['content'] = page_content
        else:
            page_info['content'] = ''
        
        if page_info.get('content') and len(page_info.get('content', '').strip()) > 50:
            parsed_pages.append(page_info)
    
    return parsed_pages

def count_tokens(text):
    """ƒê·∫øm s·ªë tokens trong text."""
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def chunk_large_text(text, max_tokens=6000):
    """Chia text l·ªõn th√†nh c√°c chunks nh·ªè ƒë∆°n gi·∫£n."""
    encoding = tiktoken.get_encoding("cl100k_base")
    tokens = encoding.encode(text)
    
    if len(tokens) <= max_tokens:
        return [text]  # Kh√¥ng c·∫ßn chunk
    
    chunks = []
    start = 0
    
    while start < len(tokens):
        # L·∫•y chunk v·ªõi max_tokens
        end = min(start + max_tokens, len(tokens))
        chunk_tokens = tokens[start:end]
        chunk_text = encoding.decode(chunk_tokens)
        chunks.append(chunk_text)
        
        # Di chuy·ªÉn start kh√¥ng overlap
        start = end
    
    return chunks

def get_embedding_single(text):
    """Get embedding cho 1 text t·ª´ OpenAI API."""
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-large"
    )
    return response.data[0].embedding

def get_embedding_batch(texts):
    """Get embedding for a batch of texts from OpenAI API."""
    # Ki·ªÉm tra t·ªïng tokens trong batch
    total_tokens = sum(count_tokens(text) for text in texts)
    if total_tokens > 8000:  # An to√†n v·ªõi gi·ªõi h·∫°n 8192
        print(f"    ‚ö†Ô∏è  C·∫£nh b√°o: Batch c√≥ {total_tokens:,} tokens")
    
    response = client.embeddings.create(
        input=texts,
        model="text-embedding-3-large"
    )
    return [item.embedding for item in response.data]

def calculate_embedding_cost(total_tokens):
    """T√≠nh to√°n chi ph√≠ embedding.
    text-embedding-3-large: $0.00013 / 1K tokens
    """
    cost_per_1k_tokens = 0.00013  # USD cho text-embedding-3-large
    total_cost = (total_tokens / 1000) * cost_per_1k_tokens
    return total_cost

def main():
    """Main function - l√†m m·ªçi th·ª© t·ª± ƒë·ªông."""
    print("="*70)
    print("üöÄ SENTIA AI RAG - MILVUS UPDATE TOOL")
    print("="*70)
    print("üîÑ T·ª± ƒë·ªông: Drop collection c≈© ‚Üí T·∫°o m·ªõi ‚Üí Update data ‚Üí T·∫°o index")
    print("="*70)
    
    # Connect to Milvus
    connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)
    
    collection_name = "sentia_website"
    
    # üóëÔ∏è B∆Ø·ªöC 1: X√≥a collection c≈© n·∫øu c√≥
    if utility.has_collection(collection_name):
        print(f"üóëÔ∏è  T√¨m th·∫•y collection c≈©: {collection_name}")
        utility.drop_collection(collection_name)
        print(f"‚úÖ ƒê√£ x√≥a collection c≈©")
    else:
        print(f"‚ÑπÔ∏è  Ch∆∞a c√≥ collection: {collection_name}")
    
    # üÜï B∆Ø·ªöC 2: T·∫°o collection m·ªõi v·ªõi schema ƒë·∫ßy ƒë·ªß
    print(f"üÜï ƒêang t·∫°o collection m·ªõi v·ªõi schema ƒë·∫ßy ƒë·ªß...")
    from pymilvus import CollectionSchema, FieldSchema, DataType
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=3072),
        FieldSchema(name="page_title", dtype=DataType.VARCHAR, max_length=500),
        FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=500),
        FieldSchema(name="page_number", dtype=DataType.INT64),
        FieldSchema(name="page_length", dtype=DataType.VARCHAR, max_length=100),
        FieldSchema(name="chunk_index", dtype=DataType.INT64),
        FieldSchema(name="is_chunked", dtype=DataType.BOOL)
    ]
    schema = CollectionSchema(fields=fields)
    collection = Collection(name=collection_name, schema=schema)
    print(f"‚úÖ ƒê√£ t·∫°o collection: {collection_name} (8 fields, dimension: 3072)")
    
    # üìÇ B∆Ø·ªöC 3: ƒê·ªçc file d·ªØ li·ªáu
    crawl_file = Path("sentia_full_website.txt")
    if not crawl_file.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {crawl_file}")
        return
    
    print(f"üìÇ ƒêang ƒë·ªçc file: {crawl_file}")
    pages = parse_website_crawl_file(crawl_file)
    print(f"üìÑ ƒê√£ parse {len(pages)} trang t·ª´ file crawl")
    
    # üìä B∆Ø·ªöC 4: X·ª≠ l√Ω d·ªØ li·ªáu
    total_tokens_used = 0
    successful_pages = 0
    failed_pages = 0
    
    # X·ª≠ l√Ω theo batches 3 trang/l·∫ßn ƒë·ªÉ tr√°nh token limit
    batch_size = 3
    total_batches = (len(pages) + batch_size - 1) // batch_size
    
    print(f"üîÑ S·∫Ω x·ª≠ l√Ω {total_batches} batches, m·ªói batch t·ªëi ƒëa {batch_size} trang")
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(pages))
        batch_pages = pages[start_idx:end_idx]
        
        print(f"\n--- Batch {batch_idx + 1}/{total_batches} ---")
        print(f"X·ª≠ l√Ω trang {start_idx + 1} ƒë·∫øn {end_idx} ({len(batch_pages)} trang)")
        
        # Prepare data for this batch
        batch_contents = []
        batch_page_titles = []
        batch_urls = []
        batch_page_numbers = []
        batch_page_lengths = []
        batch_chunk_indices = []
        batch_is_chunked = []
        
        for page in batch_pages:
            content = page.get('content', '')
            if content and len(content.strip()) > 50:
                page_tokens = count_tokens(content)
                
                # Ki·ªÉm tra n·∫øu trang qu√° l·ªõn c·∫ßn chunk
                if page_tokens > 6500:  # Ng∆∞·ª°ng ƒë∆°n gi·∫£n
                    print(f"    üìÑ Trang {page.get('page_number', 'N/A')} ({page_tokens:,} tokens) - chia chunks")
                    chunks = chunk_large_text(content, max_tokens=6000)
                    print(f"      ‚îî‚îÄ Chia th√†nh {len(chunks)} chunks")
                    
                    for chunk_idx, chunk in enumerate(chunks):
                        batch_contents.append(chunk)
                        batch_page_titles.append(page.get('page_title', 'Unknown'))
                        batch_urls.append(page.get('url', ''))
                        batch_page_numbers.append(page.get('page_number', 0))
                        batch_page_lengths.append(page.get('length', ''))
                        batch_chunk_indices.append(chunk_idx)
                        batch_is_chunked.append(True)  # ƒê√°nh d·∫•u ƒë√£ chia chunk
                else:
                    batch_contents.append(content)
                    batch_page_titles.append(page.get('page_title', 'Unknown'))
                    batch_urls.append(page.get('url', ''))
                    batch_page_numbers.append(page.get('page_number', 0))
                    batch_page_lengths.append(page.get('length', ''))
                    batch_chunk_indices.append(0)  # Chunk ƒë·∫ßu ti√™n cho trang kh√¥ng chia
                    batch_is_chunked.append(False)  # Kh√¥ng chia chunk
        
        if not batch_contents:
            print("  - Kh√¥ng c√≥ n·ªôi dung h·ª£p l·ªá trong batch n√†y")
            continue
        
        # T√≠nh t·ªïng tokens trong batch n√†y
        batch_tokens = sum(count_tokens(content) for content in batch_contents)
        total_tokens_used += batch_tokens
        
        print(f"  - ƒêang t·∫°o embedding cho {len(batch_contents)} items ({batch_tokens:,} tokens)...")
        
        try:
            # T·∫°o embeddings cho c·∫£ batch
            embeddings = get_embedding_batch(batch_contents)
            print(f"  - ‚úÖ Batch th√†nh c√¥ng: {len(embeddings)} embeddings")
            
            # Insert into Milvus
            entities = [
                batch_contents,
                embeddings,
                batch_page_titles,
                batch_urls,
                batch_page_numbers,
                batch_page_lengths,
                batch_chunk_indices,
                batch_is_chunked
            ]
            
            insert_result = collection.insert(entities)
            print(f"  - ‚úÖ ƒê√£ ch√®n {len(batch_contents)} items v√†o Milvus")
            
            successful_pages += len(batch_contents)
            
            # Flush ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c ghi
            collection.flush()
            
        except Exception as e:
            print(f"  - ‚ùå Batch {batch_idx + 1} l·ªói: {e}")
            print(f"  - üîÑ Fallback: X·ª≠ l√Ω t·ª´ng item m·ªôt...")
            
            # Fallback: X·ª≠ l√Ω t·ª´ng item m·ªôt c√°ch ƒë∆°n gi·∫£n
            for i, (content, title, url, page_num, length, chunk_idx, is_chunked) in enumerate(zip(
                batch_contents, batch_page_titles, batch_urls, batch_page_numbers, batch_page_lengths, batch_chunk_indices, batch_is_chunked
            )):
                try:
                    content_tokens = count_tokens(content)
                    
                    # N·∫øu v·∫´n qu√° l·ªõn, chia ƒë∆°n gi·∫£n
                    if content_tokens > 6500:
                        print(f"    üìÑ Item qu√° l·ªõn ({content_tokens:,} tokens) - chia nh·ªè...")
                        mini_chunks = chunk_large_text(content, max_tokens=5500)
                        
                        for mini_idx, mini_chunk in enumerate(mini_chunks):
                            try:
                                embedding = get_embedding_single(mini_chunk)
                                
                                # Insert mini chunk
                                entities = [
                                    [mini_chunk],
                                    [embedding], 
                                    [title],
                                    [url],
                                    [page_num],
                                    [length],
                                    [mini_idx],
                                    [True]  # Lu√¥n True v√¨ ƒë√£ chia
                                ]
                                
                                insert_result = collection.insert(entities)
                                collection.flush()
                                successful_pages += 1
                                print(f"      ‚úÖ Mini-chunk {mini_idx + 1}/{len(mini_chunks)}")
                                
                            except Exception as mini_error:
                                print(f"      ‚ùå L·ªói: {mini_error}")
                                failed_pages += 1
                                continue
                    else:
                        print(f"    üìÑ Item {page_num}: {title[:30]}... ({content_tokens:,} tokens)")
                        embedding = get_embedding_single(content)
                        
                        # Insert item
                        entities = [
                            [content],
                            [embedding], 
                            [title],
                            [url],
                            [page_num],
                            [length],
                            [chunk_idx],
                            [is_chunked]
                        ]
                        
                        insert_result = collection.insert(entities)
                        collection.flush()
                        
                        successful_pages += 1
                        print(f"    ‚úÖ Th√†nh c√¥ng")
                    
                except Exception as page_error:
                    print(f"    ‚ùå L·ªói: {page_error}")
                    failed_pages += 1
                    continue
        
        # Delay 1s gi·ªØa c√°c batch
        if batch_idx < total_batches - 1:
            print("  - üí§ Delay 1s...")
            time.sleep(1)
    
    # üîß B∆Ø·ªöC 5: T·∫°o index t·ª± ƒë·ªông
    print(f"\nüîß ƒêang t·∫°o index cho vector search...")
    index_params = {
        "metric_type": "L2",
        "index_type": "IVF_FLAT",
        "params": {"nlist": 1024}
    }
    collection.create_index(field_name="embedding", index_params=index_params)
    print(f"‚úÖ ƒê√£ t·∫°o index th√†nh c√¥ng")
    
    # üöÄ B∆Ø·ªöC 6: Load collection ƒë·ªÉ s·ª≠ d·ª•ng
    collection.load()
    print(f"üöÄ Collection ƒë√£ ƒë∆∞·ª£c load v√† s·∫µn s√†ng s·ª≠ d·ª•ng")
    
    # üí∞ B∆Ø·ªöC 7: T√≠nh to√°n chi ph√≠ v√† b√°o c√°o
    total_cost_usd = calculate_embedding_cost(total_tokens_used)
    total_cost_vnd = total_cost_usd * 24000  # T·ª∑ gi√° g·∫ßn ƒë√∫ng
    
    # B√°o c√°o k·∫øt qu·∫£
    print(f"\n" + "="*70)
    print(f"üéâ HO√ÄN TH√ÄNH! SENTIA AI RAG ƒê√É S·∫¥N S√ÄNG!")
    print(f"="*70)
    print(f"üìä Th·ªëng k√™ x·ª≠ l√Ω:")
    print(f"   ‚Ä¢ T·ªïng s·ªë trang g·ªëc: {len(pages)}")
    print(f"   ‚Ä¢ Items ƒë√£ x·ª≠ l√Ω: {successful_pages}")
    print(f"   ‚Ä¢ Th·∫•t b·∫°i: {failed_pages}")
    print(f"   ‚Ä¢ T·ª∑ l·ªá th√†nh c√¥ng: {successful_pages/(successful_pages+failed_pages)*100:.1f}%")
    print(f"   ‚Ä¢ Documents trong DB: {collection.num_entities}")
    print(f"\nüí∞ Chi ph√≠ embedding:")
    print(f"   ‚Ä¢ T·ªïng tokens: {total_tokens_used:,}")
    print(f"   ‚Ä¢ Model: text-embedding-3-large")
    print(f"   ‚Ä¢ T·ªïng chi ph√≠: ${total_cost_usd:.4f} USD")
    print(f"   ‚Ä¢ Quy ƒë·ªïi VND: {total_cost_vnd:,.0f} ‚Ç´")
    if successful_pages > 0:
        print(f"   ‚Ä¢ Chi ph√≠/item: ${total_cost_usd/successful_pages:.6f} USD")
    print(f"\nüéØ Database ready for AI RAG!")
    print(f"   ‚Ä¢ Collection: {collection_name}")
    print(f"   ‚Ä¢ Status: Loaded & Indexed")
    print(f"   ‚Ä¢ Ready for vector search")
    print(f"="*70)

if __name__ == "__main__":
    main()
